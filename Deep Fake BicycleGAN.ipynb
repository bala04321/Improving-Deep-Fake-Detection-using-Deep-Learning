{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "def get_data():\n",
    "    return pd.read_csv('../input/deepfake-faces/metadata.csv')\n",
    "meta=get_data()\n",
    "meta.head()\n",
    "meta.shape\n",
    "len(meta[meta.label=='FAKE']),len(meta[meta.label=='REAL'])\n",
    "real_df = meta[meta[\"label\"] == \"REAL\"]\n",
    "fake_df = meta[meta[\"label\"] == \"FAKE\"]\n",
    "sample_size = 8000\n",
    "\n",
    "real_df = real_df.sample(sample_size, random_state=42)\n",
    "fake_df = fake_df.sample(sample_size, random_state=42)\n",
    "\n",
    "sample_meta = pd.concat([real_df, fake_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train_set, Test_set = train_test_split(sample_meta,test_size=0.2,random_state=42,stratify=sample_meta['label'])\n",
    "Train_set, Val_set  = train_test_split(Train_set,test_size=0.3,random_state=42,stratify=Train_set['label'])\n",
    "Train_set.shape,Val_set.shape,Test_set.shape\n",
    "((8960, 5), (3840, 5), (3200, 5))\n",
    "trace0 = go.Bar(\n",
    "    x=['Train Set', 'Validation Set', 'Test Set'],\n",
    "    y=y[0],\n",
    "    name='REAL',\n",
    "    marker=dict(color='#33cc33'),\n",
    "    opacity=0.7,\n",
    "    text=y[0],  # Adding hover text\n",
    "    hoverinfo='text+y'\n",
    ")\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x=['Train Set', 'Validation Set', 'Test Set'],\n",
    "    y=y[1],\n",
    "    name='FAKE',\n",
    "    marker=dict(color='#ff3300'),\n",
    "    opacity=0.7,\n",
    "    text=y[1],  # Adding hover text\n",
    "    hoverinfo='text+y'\n",
    ")\n",
    "\n",
    "data = [trace0, trace1]\n",
    "layout = go.Layout(\n",
    "    title='Count of classes in each set',\n",
    "    xaxis={'title': 'Set'},\n",
    "    yaxis={'title': 'Count'},\n",
    "    barmode='group'  # Optional: specify the bar mode\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,15))\n",
    "for cur,i in enumerate(Train_set.index[25:50]):\n",
    "    plt.subplot(5,5,cur+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    \n",
    "    plt.imshow(cv2.imread('../input/deepfake-faces/faces_224/'+Train_set.loc[i,'videoname'][:-4]+'.jpg'))\n",
    "    \n",
    "    if(Train_set.loc[i,'label']=='FAKE'):\n",
    "        plt.xlabel('FAKE Image')\n",
    "    else:\n",
    "        plt.xlabel('REAL Image')\n",
    "        \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retreive_dataset(set_name):\n",
    "    images,labels=[],[]\n",
    "    for (img, imclass) in zip(set_name['videoname'], set_name['label']):\n",
    "        images.append(cv2.imread('../input/deepfake-faces/faces_224/'+img[:-4]+'.jpg'))\n",
    "        if(imclass=='FAKE'):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    \n",
    "    return np.array(images),np.array(labels)\n",
    "\n",
    "X_train,y_train=retreive_dataset(Train_set)\n",
    "X_val,y_val=retreive_dataset(Val_set)\n",
    "X_test,y_test=retreive_dataset(Test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, LeakyReLU, Flatten, Dense, Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_generator(input_shape=(224, 224, 3), output_shape=(224, 224, 3), latent_dim=100):\n",
    "    input_image = Input(shape=input_shape, name='input_image')\n",
    "    \n",
    "    # Encoder\n",
    "    x = Conv2D(64, kernel_size=4, strides=2, padding='same')(input_image)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(128, kernel_size=4, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    # Flatten for latent representation\n",
    "    x = Flatten()(x)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "    \n",
    "    # Sampler\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    \n",
    "    # Decoder\n",
    "    x = Dense(56*56*128)(z)\n",
    "    x = Reshape((56, 56, 128))(x)\n",
    "    x = Conv2DTranspose(64, kernel_size=4, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='sigmoid')(x)\n",
    "\n",
    "    return Model(input_image, x)\n",
    "\n",
    "def build_discriminator(input_shape=(224, 224, 3)):\n",
    "    input_image = Input(shape=input_shape, name='input_image')\n",
    "    \n",
    "    x = Conv2D(64, kernel_size=4, strides=2, padding='same')(input_image)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(128, kernel_size=4, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    validity = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(input_image, validity)\n",
    "\n",
    "def build_bicyclegan(generator, discriminator):\n",
    "    input_image = Input(shape=(224, 224, 3), name='input_image')\n",
    "    \n",
    "    fake_image = generator(input_image)\n",
    "    validity_real = discriminator(input_image)\n",
    "    validity_fake = discriminator(fake_image)\n",
    "    \n",
    "    return Model(input_image, [validity_real, validity_fake, fake_image])\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "bicycle_gan = build_bicyclegan(generator, discriminator)\n",
    "bicycle_gan.compile(optimizer='adam',\n",
    "                    loss=['binary_crossentropy', 'binary_crossentropy', 'mse'],\n",
    "                    loss_weights=[1, 1, 10],  # Adjust weights for different losses\n",
    "                    metrics=['accuracy'])\n",
    "bicycle_gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define loss functions\n",
    "binary_crossentropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define metrics\n",
    "accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "# Define number of epochs and batch size\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Training loop\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_start in range(0, len(X_train), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(X_train))\n",
    "        real_images_batch = X_train[batch_start:batch_end]\n",
    "        \n",
    "        with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
    "            # Generate fake images\n",
    "            generated_images = generator(real_images_batch, training=True)\n",
    "            \n",
    "            # Generate fake labels\n",
    "            fake_labels_batch = np.zeros((len(real_images_batch), 1))  # Assuming all generated images are fake\n",
    "            \n",
    "            # Discriminator loss\n",
    "            real_loss = binary_crossentropy(y_train[batch_start:batch_end, np.newaxis], discriminator(real_images_batch, training=True))\n",
    "            fake_loss = binary_crossentropy(fake_labels_batch, discriminator(generated_images, training=True))\n",
    "            total_discriminator_loss = real_loss + fake_loss\n",
    "            \n",
    "            # Generator loss\n",
    "            generator_loss = binary_crossentropy(y_train[batch_start:batch_end, np.newaxis], discriminator(generated_images, training=True))\n",
    "            reconstruction_loss = mse_loss(real_images_batch, generated_images)\n",
    "            total_generator_loss = generator_loss + 10 * reconstruction_loss  # Adjust weights as needed\n",
    "            \n",
    "        # Calculate gradients\n",
    "        discriminator_gradients = disc_tape.gradient(total_discriminator_loss, discriminator.trainable_variables)\n",
    "        generator_gradients = gen_tape.gradient(total_generator_loss, generator.trainable_variables)\n",
    "        \n",
    "        # Apply gradients\n",
    "        optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "        \n",
    "        # Update metrics\n",
    "        accuracy.update_state(y_train[batch_start:batch_end], discriminator(real_images_batch, training=True))\n",
    "        \n",
    "    # Print progress\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Discriminator Loss: {total_discriminator_loss}, '\n",
    "          f'Generator Loss: {total_generator_loss}, Accuracy: {accuracy.result()}')\n",
    "\n",
    "    # Reset metrics at the end of each epoch\n",
    "    accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs+1), discriminator_losses, label='Discriminator Loss')\n",
    "plt.plot(range(1, epochs+1), generator_losses, label='Generator Loss')\n",
    "plt.plot(range(1, epochs+1), accuracies, label='Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.title('Training Results')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
